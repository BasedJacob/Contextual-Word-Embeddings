{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neuralmt: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(build=False)\n",
    "model.load(os.path.join('data', 'seq2seq_E049.pt'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# loading test dataset\n",
    "test_iter = loadTestData(os.path.join('data', 'input', 'dev.txt'), model.fields['src'],\n",
    "                            device=device, linesToLoad=sys.maxsize)\n",
    "results = translate(model, test_iter) # Warning: will take >5mins depending on your machine\n",
    "print(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bleu_check import bleu\n",
    "ref_t = []\n",
    "with open(os.path.join('data','reference','dev.out')) as r:\n",
    "    ref_t = r.read().strip().splitlines()\n",
    "print(bleu(ref_t, results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "For the baseline implementation of adding attention to the sequence to sequence model, the only changes to the default solution is to the calculate alpha, and forward methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAlpha(self, decoder_hidden, encoder_out):\n",
    "        \"\"\"\n",
    "        param encoder_out: (seq, batch, dim),\n",
    "        param decoder_hidden: (seq, batch, dim)\n",
    "        \"\"\"\n",
    "        scores = torch.tanh(self.W_enc(encoder_out)+ self.W_dec(decoder_hidden))\n",
    "        out = torch.transpose(self.V_att(scores),0,1)\n",
    "        alpha = torch.nn.functional.softmax(out, dim=1)\n",
    "        return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the alpha function, we are calculating the weights for additive attention. There was a mismatch with dimensions after the tanh function that was patched with transposing the output before putting it into the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, decoder_hidden, encoder_out):\n",
    "        \"\"\"\n",
    "        encoder_out: (seq, batch, dim),\n",
    "        decoder_hidden: (seq, batch, dim)\n",
    "        \"\"\"\n",
    "        alpha = self.calcAlpha(decoder_hidden, encoder_out)\n",
    "        seq, _, dim = encoder_out.shape\n",
    "        combined = torch.bmm(torch.transpose(alpha,0,1),encoder_out)\n",
    "        context = (torch.sum(combined, dim=0)).reshape(1, 1, dim)\n",
    "        return context, alpha.permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calculating the context that will be passed forward there was another mismatch of dimensions from the calcAlpha output so it is transposed again before multiplying it with the encoder hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Code\n",
    "There were some setup issues getting this to run on my own machine but it may work fine on other machines. Versions of torchtext later than 0.8.1 discontinued some functions and will not work. Also, the older version of torch was too old to be able to run off my GPU as it uses CUDA capability sm_86 which the older version of torch supports up to sm_75. Of which I had to manually change the code, in neuralmt.py line 48 into line 49, to force it to run on my cpu. Other than that, the file needs to be fed an input, which for development the file \"seq2seq_E049.pt\" was directly put into the data folder. Then with the inputs in the data folder, you can run the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 neuralmt.py > output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which should take a few minutes to run but there will be an idicator in the command window and the output will be saved to the output.txt file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "After implementing additive attention to the model the BLEU score increased substantially to 17.11 from the default score of 3.35. It was difficult getting the dimensions of all the tensors to match and also choosing the right function to multiply tensors together as there are many ways to multiply tensors such as (*,torch.matmul,@,bmm) and also passing a tensor into the nn.linear.\n",
    "\n",
    "Given the prevalence of <unk> tokens even in the baseline output, unknown word replacement would have been an especially suitable extension to the baseline solution by replacing all out-of-vocabulary words from the <unk> token with dictionary-translated words, as outlined in Sutskever et al.â€™s (2015) paper. Although we anticipate that this extension would have further increased the BLEU score of our current solution, a potential shortcoming of using dictionary replacements would be identifying word senses if the context of the sentence was not fully understood. Unfortunately, due to time constraints the other potential extensions such as beam search encoding and ensemble encoding remained unattempted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
